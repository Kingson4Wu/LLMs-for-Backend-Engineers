---
sidebar_label: 'AI系统工程实践：去神话指南'
sidebar_position: 4
---

# AI系统工程实践：去神话指南

## "AI 新概念"的分布式系统翻译表

下面这张表，你可以反复用来"去魅"。

| AI 术语 | 工程本质 |
|--------|---------|
| Agent | 有控制流的 LLM Worker |
| Planning | 语言版状态机 |
| Memory | 外挂存储 + 检索 |
| Reflection | Retry + Feedback |
| Tool Use | RPC + Schema |
| Sub-agent | 任务拆分 |
| Multi-agent | 并行采样 |

### 一句话总结：
**模型负责"决策"，系统负责"一切可靠性"**。

如果一个方案解释不清：
- 状态存在哪？
- 控制流谁管？
- 失败如何回滚？

那它要么是营销词，要么还没工程化。

## 哪些地方用 LLM 是加分，哪些是灾难

### 真正值钱的场景

1. **模糊意图 → 结构化决策**
   - 用户输入的自然语言需求
   - 模糊、不完整、上下文依赖强
   - 很难用 if/else 或规则覆盖
   - 把"人脑里那套判断"外包给模型

2. **高维规则压缩**
   - 当系统里出现上百条规则
   - 各种"但如果……"
   - 规则经常被产品改
   - 把"规则集"变成"隐式概率模型"

3. **内容理解 / 总结 / 转写**
   - 长文总结
   - 日志解释
   - 错误原因描述
   - 用户输入改写
   - 输出是"辅助信息"，出错可接受

### 应该警惕的场景

1. **核心业务状态修改**
2. **强一致性需求**
3. **需要严格可复现行为**
4. **长时间自主运行的 Autonomous Agent**

这里用 LLM，不是"先进"，而是不可控。

## 为什么 AGI 叙事对工程是危险的

AGI 讨论混合了：
- 哲学问题
- 认知科学
- 统计模型能力

但工程系统只关心一件事：**它失败时，会不会把系统一起带走？**

AGI 回答不了这个问题。

### 工程应该关心的指标
而不是"像不像人"，而是：

- **决策覆盖率**：能处理多少原本需要人工判断的情况
- **失败半径**：模型出错时，影响范围多大
- **可观测性**：能否记录输入、输出、版本，能否复现问题
- **可回滚性**：是否有版本控制和回滚机制
- **成本上限**：最坏情况下的 token 消耗是否可预测、可封顶

不能被治理的"智能"，不是能力，是风险。

## 一个后端工程师应有的最终认知

如果把全文压缩成一句话：

> **LLM 是一个强大的"不确定性压缩器"，不是一个"会思考的实体"**。

真正的系统智能，来自：
- 清晰的边界
- 严格的控制流
- 明确的失败路径
- 人对系统的理解

把希望寄托在模型"进化"，是工程上的不负责任。

## 一个工程判断表

你可以用这张表快速判断是否使用LLM：

| 问题 | 是 | 否 |
|------|----|----|
| 输入是自然语言或高度模糊？ | ✅ | ❌ |
| 规则数量正在爆炸？ | ✅ | ❌ |
| 允许概率性错误？ | ✅ | ❌ |
| 有兜底和回滚机制？ | ✅ | ❌ |
| 结果不直接修改核心状态？ | ✅ | ❌ |

- 全是"是" → 用 LLM
- 有 2 个"否" → 谨慎
- 3 个以上"否" → 别用

## 结语：系统设计的底线原则

你可以把这条原则当成红线：

> **任何不能被版本化、回滚、监控、限流的"智能"，都不应该进入核心业务路径**。

无论它叫：
- AGI
- Agent
- Autonomous
- Self-improving

当你看到：
- "模型在反思"
- "Agent 自主进化" 
- "涌现出智能"

你应该下意识问三个问题：
1. 状态在哪？
2. 控制流谁管？
3. 出错怎么办？

问不出来的，一律当营销。