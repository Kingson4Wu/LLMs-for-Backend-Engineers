**单词向量化原理与训练流程简明总结**

好的，我帮你把**单词向量化的原理**做一个完整、简明的总结，把前面讲的所有关键点串起来：

---

## **单词向量化（Word Embedding）原理总结**

### **1. 核心思想：分布式假设**

* 单词本身没有固定含义
* 单词的语义由它周围的上下文决定
* 类似单词在相似上下文中出现 → 它们的向量也应该相似

---

### **2. 向量表示**

* 每个单词被表示成一个长度为 (d) 的实数向量
* 维度 (d) 常见 50~300（经典 Word2Vec）或更高（BERT 768+）
* 向量每一维表示潜在语义特征，值通过训练学习得出

---

### **3. 上下文（Context）**

* 定义：中心词周围的前后若干个单词
* 窗口大小 (w) 是超参数：

  * 小窗口（1~2）：捕捉语法信息
  * 中窗口（5）：兼顾语法与语义
  * 大窗口（10+）：捕捉长程语义

---

### **4. 训练方法**

1. **预测方法（Word2Vec）**

   * **Skip-gram**：中心词 → 预测上下文
   * **CBOW**：上下文 → 预测中心词
2. **统计方法（共现矩阵 + SVD）**

   * 计算单词与上下文共现概率 → 降维得到向量

**训练逻辑（以Skip-gram为例）：**

* 输入中心词 → 隐藏层 → 输出预测上下文词
* 比较预测概率和实际上下文 → 计算损失 → 梯度下降
* 更新输入/输出矩阵 → 所有词向量逐渐收敛到语义合理的值
* 所有词向量在训练中**并行或交替更新**

---

### **5. 向量间相似度**

* 用 **余弦相似度** 衡量：
  [
  \cos(\theta) = \frac{\vec{v_1}\cdot \vec{v_2}}{|\vec{v_1}||\vec{v_2}|}
  ]
* 语义相近的词 → 向量方向接近 → 余弦相似度接近 1

---

### **6. 小结流程**

1. 准备语料 → 建立词表
2. 初始化向量（随机）
3. 遍历语料，中心词 + 上下文 → 训练 → 更新向量
4. 多轮迭代，向量收敛
5. 得到每个单词的最终 embedding，可用于：

   * 计算相似度
   * 文本分类、情感分析、机器翻译等下游任务

---

如果你需要，我可以画一张**完整的流程图**，展示从**语料 → one-hot → 隐藏层 → 向量 → 上下文预测 → 更新矩阵 → 最终词向量**的整个训练原理。

你希望我画吗？

