# AI 基础知识从-1到0.1：带你走进机器学习的世界
+ <https://mp.weixin.qq.com/s/1D1zGB5_yWvfkZEo3tsaEA>

```
人工智能、机器学习、深度学习与大模型

人工智能（AI）：人工智能指的是计算机执行各种决策任务的能力合集，这些任务通常模拟人类智能，例如理解自然语言、识别图像、解决问题和进行推理。AI 的目标是开发能够自主学习和适应的系统，以提升效率和准确性。

机器学习（ML）：机器学习是人工智能的一个子集，专注于如何让计算机基于数据做出决策。机器学习的主要目标是从现有的数据中发现模式和规律，并利用这些模式对未来的数据进行预测。

深度学习（DL）：深度学习是机器学习的一个领域，利用称为“神经网络”的多层结构来处理复杂数据模式识别，相对于普通机器学习，尤其擅长图像识别、自然语言处理等任务。


模型就是机器学习、深度学习中从数据中学习到的、用于做预测或决策的规则集合，这个学习过程称之为训练，这些规则通过训练过程从历史数据中提取模式和关系，使模型能够在面对新的、未见过的数据时做出靠谱的预测或决策。

而大模型是指在深度学习中拥有大量参数（后面再解释何为参数）和复杂结构的模型，这些模型通常需要大量的数据和强大的计算资源来训练，能够处理更复杂的任务并理解更深层次的数据模式。
```

# AI 基础知识从0.1到0.2——用“房价预测”入门机器学习全流程
+ https://mp.weixin.qq.com/s/Jcwfc94Ew8grjrj2guNw7w

+ 一旦模型表现满意，就可以将其部署为服务，使其能够接受新数据并返回预测结果。同时，建立监控机制，确保模型在实际应用中的性能。

# AI 基础知识从 0.2 到 0.3——构建你的第一个深度学习模型
+ https://mp.weixin.qq.com/s/IcKixETtMYftkx4ZX0r7aw

+ 深度学习核心是通过构建和训练多层神经网络（深度神经网络）模拟人脑的复杂决策能力，让计算机能够从大量数据中自主学习复杂的特征和规律，以处理和分析图像、语音、文本等非结构化数据。

+ 使用深度学习识别手写数字

```
每层中神经元的数据量是怎么决定的？
输入层：根据数据特征数量设置神经元。例如 28x28 的图像展平后有 784 个特征，所以输入层有784个神经元。
隐藏层：神经元数量一般通过经验和试验确定，前层神经元数量相对较多，用于学习更具体的局部特征，后层减少神经元数量，迫使模型抽象特征，降低模型复杂度，减少过拟合风险。
输出层：输出层神经元的数量由具体的任务和输出的类别数量决定
分类任务：在多分类问题中输出层神经元的数量等于类别的数量。例如手写数字识别任务，要识别的数字有 0 - 9 共 10 个类别，所以输出层就设置 10 个神经元

回归任务：输出层神经元的数量通常为 1 个。例如预测房价，只需要一个预测值，输出层就设置 1 个神经元。

激活函数
激活函数（Activation Function） 是神经网络中每个神经元输出时应用的非线性变换函数。它决定了一个神经元是否被激活，以及传递给下一个神经元的信息量。若没有激活函数，多层神经网络将退化为线性模型，无法学习复杂的非线性关系。有几个常用的激活函数：

ReLU：输出范围在 [0, ∞) 之间，计算简单且高效，能够有效缓解梯度消失问题，是当前最常用的隐藏层激活函数。

Sigmoid：输出范围在 (0, 1) 之间，将输入映射为 0 到 1 之间的概率值，常用于二分类问题的输出层。

Softmax：每个元素在 (0, 1) 之间，且所有元素之和为 1，通常用于多分类任务的输出层，用于表示每个类别的概率。
```

```
Transformer 与 Tensorflow

Transformer 是基于自注意力机制的神经网络架构，主要用于自然语言处理（NLP）等序列建模任务，本质上是一种解决问题的设计理念与方案。

TensorFlow 是由 Google 开发的开源机器学习框架 ，提供了丰富的工具和库来构建、训练和部署各种机器学习模型，本质上是一个工具实现。

可以利用 TensorFlow 提供的操作、函数和类来实现 Transformer 架构，进行模型训练和推理。工程同学也能简单把 Transformer 和 Tensorflow 的关系理解为 MVC 架构与 Spring MVC 的关系。
```

```
Tensorflow 与 PyTorch

TensorFlow 与 PyTorch 是当前深度学习领域中最为流行的两个开源框架：

TensorFlow 由 Google Brain 团队于 2015 年 11 月发布，原生支持分布式计算，适合大规模模型和数据的训练。在 TensorFlow 1.x 时代，静态计算图的概念对于初学者来说较为复杂。2017 年发布 TensorFlow 2.0，强调易用性与灵活性，集成了 Keras 高级 API，支持动态图模式，提高了用户体验。

PyTorch 由 Facebook 于 2016 年 1 月发布，旨在为研究人员提供一个灵活且高效的深度学习框架。发布初版采用动态计算图，使得模型构建和调试过程更加直观和简便，尤其适合研究和快速原型开发。2018 年发布 PyTorch 1.0，标志着框架的成熟，增强了生产部署能力。

两者各有优势，TensorFlow 更加适合需要强大部署能力和完整生态系统的工业应用，而 PyTorch 则因其灵活性和易用性，深受研究人员和开发者的青睐，超过 80% 的新研究论文优先使用 PyTorch 实现，虽然文中示例主要是用 Tensorflow，但如果仅仅是想了解深度学习，建议使用 PyTorch。
```

# AI 基础知识从 0.3 到 0.4——如何选对深度学习模型？
+ https://mp.weixin.qq.com/s/fTEL8KwTdXyuakLONc9vvA

+ 模型与模型架构

```
我们首先需要了解机器学习中两个关键概念——模型架构（Model Architecture）与具体模型（Trained Model）的区别。就像建筑设计中「蓝图」与「建成的房子」的关系 ——CNN 和 Transformer 是模型架构，类似于可复用的「设计蓝图」，允许开发者根据需求调整细节（如层数、参数），从而衍生出无数具体实现；而 GPT 则是基于 Transformer 蓝图构建并训练完成的「成品模型」，是前者的一个典型实例。

模型架构是模型的 “设计图纸”，决定了模型的结构和计算逻辑（无参数）。

模型是架构经过训练后的 “实例”，包含可学习的参数，能够实现具体的预测功能。

```

```
开始选择模型

拿到业务需求、收集数据、确定目标、选择模型架构，就可以开始进行模型训练了，但在机器学习开发的日常实践中，完全 从零开始训练模型的情况已越来越少，大多数开发者会基于任务需求与数据特性，优先选择在预训练模型基础上进行微调优化。这种站在巨人肩膀上的策略，既充分利用了大规模数据预训练积累的通用特征表示，又能通过轻量化的适配过程快速满足特定场景需求。

预训练模型
预训练模型（Pre-trained Models）是在特定模型架构基础上，通过在大规模通用数据集上进行初步训练，学习到通用特征或知识，设定了具体参数的神经网络模型。

Hugging Face Hub：https://huggingface.co/models

PyTorch Hub：https://pytorch.org/hub/

TensorFlow Hub：https://www.tensorflow.org/hub
```

```
微调 Fine-tuning
因为预训练模型一般使用通用数据集训练，为了更好的支持业务需求，开发者会对预训练模型进行微调。微调是指在预训练模型上，针对特定任务或特定数据集，进行少量参数的调整和训练，以使模型更好地适应新的任务需求。

随着模型规模的不断增大和应用场景的多样化，出现了多种微调方法，以提高效率、减少计算资源消耗或提升模型性能。

1. 标准微调（Standard Fine-tuning）
标准微调是最基本的微调方法，即将预训练模型在特定任务的数据集上进行全量参数的进一步训练。通常情况下，微调时会采用较低的学习率，以避免破坏预训练模型中学到的通用特征。

优点：简单直接，效果较好，尤其适用于目标任务与预训练任务相似的情况。

缺点：大模型全量微调计算资源消耗高，可能导致过拟合，尤其在目标任务数据量较小时。

2. 监督微调（Supervised Fine-Tuning, SFT）

监督微调是在标准微调基础上，利用有标注数据进行进一步训练，旨在提升模型在特定监督任务上的表现。SFT 通常应用于需要明确标签的任务，如分类、序列标注等。

自然语言处理：文本分类、命名实体识别、机器翻译等。

计算机视觉：图像分类、目标检测、图像分割等。

3. 低秩适配（Low-Rank Adaptation, LoRA）

LoRA 是一种 PEFT 的微调方法，通过在预训练模型的权重矩阵中引入低秩矩阵来适配新任务，而无需大幅调整原有模型的参数。这种方法能够显著减少微调过程中新增的参数量，同时保持或提升模型性能。

在矩阵理论中，秩（Rank）指的是矩阵中线性无关行或列的最大数量。一个矩阵的秩决定了其线性独立性的程度。

满秩矩阵：如果一个 m×n 矩阵的秩 r 等于其最小维度 min⁡(m,n)，则称其为满秩矩阵。

低秩矩阵：如果一个 m×n 矩阵的秩 r满足 r≪min⁡(m,n)，即远小于其最小维度，则称其为低秩矩阵。

低秩矩阵：在保持模型表现的同时，通过引入低秩矩阵在特定层（如 Transformer 的自注意力层）中调整权重。

参数冻结：预训练模型的原始参数冻结，仅训练新增的低秩矩阵参数。

4. 知识蒸馏（Knowledge Distillation）

知识蒸馏是通过将大型复杂模型（教师模型）的知识传递给小型高效模型（学生模型），以实现模型压缩和性能优化的技术。尽管知识蒸馏主要用于模型压缩，但它也可以作为微调的一种补充技术，进一步提升模型在特定任务上的表现。

基本原理：

教师模型：在大规模数据集上训练好的高性能模型。

学生模型：结构更简单、参数更少的模型，旨在模仿教师模型的行为。

蒸馏过程：通过让学生模型学习教师模型的输出（如软概率分布、特征表示等），传递教师模型的知识。

小小成本就能干大事，2025 年初 DeepSeek-R1 横空出世，671B 参数，训练仅消耗 278.8 万 GPU 小时，成本约为 GPT-4 的 1/200。是不是可以理解为什么 OpenAI 会公开质疑 DeepSeek 可能通过蒸馏 “窃取” 其技术成果。
```

```
Hugging face

Hugging Face提供了 模型 + 数据集 + 工具 + 社区 的一站式 AI 开发者平台，无论是想快速调用预训练模型的新手，还是需要大规模分布式训练的企业，都能在平台上找到对应的解决方案

```

```
使用预训练模型

IMDB 影评数据集包含 5 万条电影评论（2.5 万训练/2.5 万测试），标注为正面/负面情感，是情感分析领域的标杆数据集。

文本长度适中（平均 200-300 词），适合验证模型的长文本理解能力。

已完成预处理（去除 HTML 标签、统一格式），可直接使用。

接下来我们使用预训练的 bert-base-uncased 模型，对 IMDB 电影评论数据集进行情感分析，也就是判断评论是积极的还是消极的
```

```
前向传播（Forward Propagation）和反向传播（Backward Propagation）是深度学习中两个至关重要的概念，它们共同构成了神经网络模型训练的核心流程

前向传播是指将输入数据（代码中的 input_ids 和 attention_mask）依次通过神经网络的各个层，经过一系列的线性变换和非线性激活函数处理，最终得到模型的预测输出。

反向传播是在得到损失值之后，通过链式法则计算损失函数对模型中每个参数的梯度。通过计算梯度，可以知道应该如何调整模型的参数，使得损失函数的值减小，从而提高模型的性能。
```

# AI 基础知识从 0.4 到 0.5—— 计算机视觉之光 CNN
+ https://mp.weixin.qq.com/s/WsHoR7L8ZJ2QRiT2fRkIqg

```
CNN 家族模型与应用

常见模型

CNN 凭借强大的特征提取能力和对图像数据的高效处理，成为计算机视觉任务的核心技术。从早期探索到技术突破，众多 CNN 模型不断推动着领域发展
```

```
目标识别

使用 YOLO 可以快速在图像、视频中定位和分类对象。

人脸识别

使用 MTCNN 可以轻松做到人脸识别，别说，看人真准。
```

# AI 基础知识从 0.5 到 0.6—— Transformer 架构为何能统治AI领域？
+ https://mp.weixin.qq.com/s/bQE58LII4nXvO2hkjlYvsw

```
在深度学习领域，每种模型架构都有其适合解决的问题，CNN 擅长捕捉图像局部特征，RNN 专注于处理序列依赖。然而 Transformer 架构却在自然语言处理（NLP）、计算机视觉（CV）、图像生成等多个前沿领域都占据核心地位，从机器翻译、文本生成，从图像分类到多模态交互，就连 GPT 等引领行业变革的大模型也将 Transformer 架构作为根基。
```

```
seq2seq 任务

Transformer 的诞生要从 NLP 的核心—— seq2seq 任务说起，seq2seq 任务是指将一个输入序列数据转换为另一个输出序列数据，不需要两个序列具有相同的长度或结构。

序列数据是指有顺序的数据集合，比如一句话中的文字、一段语音信号等。

典型的 Seq2Seq 问题包括：

机器翻译：输入中文句子 "非常感谢"，输出英文句子 "Thank you very much"

文本摘要：输入一篇长文章，输出简短摘要

问答系统：输入一个问题，输出一个答案

聊天机器人：输入用户消息，输出回复消息

Seq2Seq 模型由两个主要组件组成：编码器（Encoder）和解码器（Decoder），其核心思想是“编码全局信息→解码逐步生成”


```

```
RNN 和 LSTM 的困境

在 Transformer 出现之前，Seq2Seq 编码器和解码器的实现主流方法是循环神经网络（RNN）和长短期记忆网络（LSTM）。
```

```
Transformer 工作流程

注意力的实现 QKV

自注意力机制使模型能够关注输入序列 token 之间的相关性，从而使其能够捕捉数据中的复杂关系和依赖。现在我们知道了什么是注意力，但具体是怎么计算的呢？这就要引入 Transformer 的核心创新：QKV 机制。

多头注意力

单个注意力已经很强大了，但 Transformer 的设计者们想：如果同时用多个"大脑"从不同角度理解同一句话会怎样？这就是多头注意力（Multi-Head Attention）——输入序列被分到多个子空间，每个子空间独立计算自己的注意力表示。

Transformer 的优势

Transformer 相较于传统的 RNN（包括 LSTM 和 GRU）为基础的 Seq2Seq 模型，在多个方面具有显著的优势：
1.并行运算

RNN 的设计是循环递归的，每个时间步的计算依赖于前一个时间步的输出。这种顺序计算方式阻碍了模型在长序列上的并行化处理，导致训练和推理过程速度较慢。

Transformer 完全摒弃了递归计算，使用掩码自注意力机制在训练阶段能够同时对整个序列进行处理，加速模型训练过程。

2.长距离依赖

RNN 越往后传播，模型的隐藏状态可能丢失早期输入的信息，尽管 LSTM、GRU 等通过“门控机制”稍微缓解了这个问题，但在捕获长距离依赖时仍显乏力。

Transformer 通过自注意力机制，每个 token 都能直接与整个序列中的所有 token 建立联系，全局感知能力极大提高了对长序列上下文的理解能力。

3.丰富的特征表示

RNN 的隐藏状态是单一维度的时间步相关表示，难以建模复杂的句法和语义模式。

多头注意力机制让 Transformer 可以从多角度捕获语义特征，更全面地表示复杂的信息依赖。

Transformer 在计算效率、长距离依赖建模、特征表示能力和任务泛化性等方面全面优于 RNN Seq2Seq，彻底颠覆了序列建模的传统方式，成为现代 NLP 和更广泛任务中的核心架构。

```

# AI 基础知识从 0.6 到 0.7—— 彻底拆解深度神经网络训练的五大核心步骤
+ https://mp.weixin.qq.com/s/1t7WmIPvDCqAcTdkaQqRpw

```
从粗粒度上上来讲，深度神经网络训练过程有 5 步：

1.前向传播

2.计算损失

3.反向传播

4.更新参数

5.循环训练
```

```
模型整个训练过程包含两个阶段：前向传播和反向传播。

阶段一：前向传播 (Forward Propagation)

1.输入数据：将一个或一批训练数据输入到网络的第一层。

2.逐层计算：数据从输入层开始，逐层向前流动。每一层的神经元接收上一层的输出，进行加权求和与激活函数计算，然后将结果传递给下一层。

3.生成输出：直到最后一层输出模型的预测结果 ŷ 。

4.计算损失：用损失函数比较模型的预测 ŷ 和真实标签 y ，计算出总的损失值 L 。

整个过程就像水流顺着管道从头流到尾，最终得到一个误差值。

阶段二：反向传播 (Backward Propagation)

梯度（误差信号）将从网络的末端开始，反向传播回网络的每一层

1.从损失函数开始：计算损失 L 对网络最后一层输出 ŷ 的梯度。这是梯度计算的起点。

2.逐层向后计算：利用链式法则，将梯度从当前层反向传播到前一层。具体来说，就是计算损失 L 对当前层所有参数（ w ,  b ）以及对当前层输入（即上一层的输出）的梯度。

3.梯度传递：上一层接收到从后一层传来的梯度信号后，继续用同样的方法计算本层的梯度，并继续向前传递。

4.到达输入层：这个过程一直持续到网络的第一层。当这个过程完成时，我们就得到了损失 L 对网络中所有参数的梯度。

整个过程就像把在终点产生的误差，按照原路“归因”或“分摊”给每一层的每一个参数，搞清楚每个参数对最终的总误差贡献了多少“责任”。

反向传播过程看起来就很数学、很复杂，幸运的是 PyTorch、TensorFlow 等深度学习框架内置了自动求导 (Autograd) 引擎，它会自动构建计算图，并利用链式法则为我们计算所有参数的梯度。我们只需要简单地调用一个函数。
```

```
梯度下降

梯度下降 (Gradient Descent) 是一种在机器学习和深度学习中广泛使用的优化算法。它的主要目标是找到使某个损失函数达到最小值的模型参数，因为希望最小化损失函数，所以我们需要沿着函数值下降最快的方向移动。


循环训练

至此，我们完成了一个完整的训练步骤（iteration）。但只走一步是远远不够的。我们需要将整个数据集过一遍，这个过程称为一个周期 (Epoch)。然后我们会重复很多个 Epoch，让模型在数据上反复学习。

在一个 Epoch 中，对于每一批（batch）数据：

1.前向传播：输入数据，得到预测值。

2.计算损失：用损失函数计算预测值和真实值之间的差距。

3.反向传播：调用loss.backward()，计算每个参数的梯度。

4.更新参数：调用optimizer.step()，使用梯度和优化算法更新参数。

这个循环不断重复，模型的参数被持续优化，损失值（Loss）会不断下降，模型在训练数据和未见过的验证数据上的表现会越来越好，直到达到收敛或满足我们设定的停止条件。
```







